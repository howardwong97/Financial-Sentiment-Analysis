{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is BERT?\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Taken directly from the abstract of the paper: \n",
    "\n",
    "> We introduce a new language representation model called BERT, which stands for\n",
    "Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications <cite data-cite=\"8277123/AADH59T3\"></cite>.\n",
    "\n",
    "Let's break it down a bit. Firstly, what is a *Transformer*?\n",
    "\n",
    "## Transformers\n",
    "\n",
    "### Self-Attention\n",
    "\n",
    "For more details, [\"Attention Is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf). Roughly speaking, a Transformer has a sequence-to-sequence encoder-decoder architecture. We'll focus on the concept of *encoders*, since this is what is relevant for BERT.\n",
    "\n",
    "<img src=\"images/bert.png\">\n",
    "\n",
    "The image above shows the key component of a Transformer - *self-attention*.\n",
    "\n",
    "> “Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.” <cite data-cite=\"8277123/8IC9ACGZ\"></cite>\n",
    "\n",
    "From the encoder's input vectors, there are three vectors of interest - Query, $Q$, Key, $K$ and Value, $V$. The attention is computed as\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{Attention}_{Q,K,V} = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "\\end{equation}\n",
    "\n",
    "We take a query vector, e.g. $(q_1, q_2, q_3)$, and perform matrix multiplication with the key vector, $(k_1, k_2, k_3)$ of all the words to get something along the lines of:\n",
    "\n",
    "\n",
    "\\begin{bmatrix}\n",
    "q_1 k_1 & q_2 k_1 & q_3 k_1 \\\\\n",
    "q_1 k_2 & q_2 k_2 & q_3 k_2 \\\\\n",
    "q_1 k_3 & q_2 k_3 & q_3 k_3\\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "\n",
    "This is the *'score'*. The columns represent the score for each query. This is then scaled by a factor $1 / \\sqrt{d_k}$, where $d_k$ refers to the dimensionality of the query/key vectors. The scaling is performed so that the arguments of the softmax function do not become excessively large with keys of higher dimensions. Subsequently, it is normalized using the softmax-activation function. Therefore, the $\\text{softmax}$ term can be viewed as *weights* that are assigned to the values in vector $(v_1, v_2, v_3)$.\n",
    "\n",
    "To summarise:\n",
    "\n",
    "> \"An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\" <cite data-cite=\"8277123/8IC9ACGZ\"></cite>\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Instead of using a single attention function, *Multi-Head Attention* does just as the name implies. It linearly projects the queries, keys and values $h$ times with different, learned linear projections to dimensions. On each of the projected versions of queries, keys and values, the attention function is performed in parallel, yielding output values that are concatenated and once again projected. This results in the final values, as depicted in the figure above.\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "In the proposed architecture thus far, there is no notion of word order. We need a position-dependent signal for each word-embedding to incorporate this contextual information. Recurrent Neural Networks (RNNs) achieve this by parsing a sentence word by word sequentially. \n",
    "\n",
    "In Transformers, \"positional encodings\" are added to the input embeddings, and they share the same dimension as the embeddings. In the work outlined by Vaswani, sine and cosine functions of different frequencies are used:\n",
    "\n",
    "\\begin{align}\n",
    "    PE_{(pos,2i)} &= \\sin \\left(pos/10000^{\\frac{2i}{d_{model}}}\\right) \\\\\n",
    "    PE_{(pos,2i+1)} &= \\cos \\left(pos/10000^{\\frac{2i}{d_{model}}}\\right)\n",
    "\\end{align}\n",
    "\n",
    "where $pos$ is the position and $i$ is the dimension.\n",
    "\n",
    "## What makes BERT special?\n",
    "\n",
    "BERT was trained in two ways - **Masked LM (MLM)** and **Next Sentence Prediction (NSP)**. The BERT model was trained for both tasks together, minimizing the combined loss of both tasks.\n",
    "\n",
    "### Masked LM\n",
    "\n",
    "In pre-training BERT, 15% of the words in each sentenced was replaced by a `[MASK]` token. The model was then trained to predict the value of the masked words. This was achieved by adding a classification layer on top of the encoder input. the output vectors are multiplied by the embedding matrix, transforming them into the vocabulary dimension. \n",
    "\n",
    "The probability of each word in the vocabulary is computed using the softmax function. BERT uses a loss function that only takes into consideration the *prediction of the masked values*, and ignores the predictions of the *non-masked words*.\n",
    "\n",
    "### Next Sentence Prediction\n",
    "\n",
    "The model received pairs of sentences as input and was trained to predict the second sentence in the pair. During training, 50% of the inputs were an actual pair and the other 50% had random (disconnected) sentences from the corpus. To distinguish between the two sentences in training, certain processing steps were taken for the inputs:\n",
    "\n",
    "1. There are two types of tokens, `[CLS]`, which is inserted at the beginning of the first sentence, and `[SEP]`, which is inserted at the end of each sentence.\n",
    "\n",
    "2. A sentence embedding indicating Sentence A or Sentence B is added to each token.\n",
    "\n",
    "3. A positional embedding is added to each token to indicate its position in the sequence.\n",
    "\n",
    "The entire input sequence first goes through the Transformer model, as described in the previous section. The output of the `[CLS]` token is transformed into a 2 x 1 shaped vector, using a simple classification layer. Then, the probability of the next sequence is computed with softmax.\n",
    "\n",
    "## Why am I using BERT?\n",
    "\n",
    "I wish to create a model that can accurately classify the sentiment of domain-specific texts. A 'Financial Phrasebank' exists on [Kaggle](https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news). However, there are only 4837 unique examples.\n",
    "\n",
    "BERT proves especially useful for classification tasks because it was trained to perform NSP tasks. We can take advantage of a pre-trained BERT model that has been trained on a vast corpus:\n",
    "\n",
    "> The original English-language BERT model comes with two pre-trained general types: (1) the BERTBASE model, a 12-layer, 768-hidden, 12-heads, 110M parameter neural network architecture, and (2) the BERTLARGE model, a 24-layer, 1024-hidden, 16-heads, 340M parameter neural network architecture; both of which were trained on the BooksCorpus with 800M words, and a version of the English Wikipedia with 2,500M words. <cite data-cite=\"8277123/AADH59T3\"></cite>.\n",
    "\n",
    "In the next notebook, I use PyTorch to fine-tune a BERT model for financial news headlines.\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "<div class=\"cite2c-biblio\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "8277123/8IC9ACGZ": {
     "URL": "http://arxiv.org/abs/1706.03762",
     "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
     "accessed": {
      "day": 15,
      "month": 10,
      "year": 2020
     },
     "author": [
      {
       "family": "Vaswani",
       "given": "Ashish"
      },
      {
       "family": "Shazeer",
       "given": "Noam"
      },
      {
       "family": "Parmar",
       "given": "Niki"
      },
      {
       "family": "Uszkoreit",
       "given": "Jakob"
      },
      {
       "family": "Jones",
       "given": "Llion"
      },
      {
       "family": "Gomez",
       "given": "Aidan N."
      },
      {
       "family": "Kaiser",
       "given": "Lukasz"
      },
      {
       "family": "Polosukhin",
       "given": "Illia"
      }
     ],
     "container-title": "arXiv:1706.03762 [cs]",
     "id": "8277123/8IC9ACGZ",
     "issued": {
      "day": 5,
      "month": 12,
      "year": 2017
     },
     "note": "arXiv: 1706.03762",
     "title": "Attention Is All You Need",
     "type": "article-journal"
    },
    "8277123/AADH59T3": {
     "URL": "http://arxiv.org/abs/1810.04805",
     "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
     "accessed": {
      "day": 15,
      "month": 10,
      "year": 2020
     },
     "author": [
      {
       "family": "Devlin",
       "given": "Jacob"
      },
      {
       "family": "Chang",
       "given": "Ming-Wei"
      },
      {
       "family": "Lee",
       "given": "Kenton"
      },
      {
       "family": "Toutanova",
       "given": "Kristina"
      }
     ],
     "container-title": "arXiv:1810.04805 [cs]",
     "id": "8277123/AADH59T3",
     "issued": {
      "day": 24,
      "month": 5,
      "year": 2019
     },
     "note": "arXiv: 1810.04805",
     "shortTitle": "BERT",
     "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
     "title-short": "BERT",
     "type": "article-journal"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": "2",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
