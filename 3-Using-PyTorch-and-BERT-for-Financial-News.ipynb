{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyTorch and BERT for Financial News\n",
    "\n",
    "## EDA\n",
    "\n",
    "A Financial Phrasebank was developed for a paper published by Malo et al <cite data-cite=\"8277123/Q3X6FHBI\"></cite>. It is available for download on [Kaggle](https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/financialnews.csv', encoding='latin1', header=None)\n",
    "df.columns = ['label', 'text']\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a rough idea of the number of examples for each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['label'].value_counts().plot(kind='bar', color=['grey', 'green', 'red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character count: 128\n",
      "Average word count: 23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "neutral     0.593595\n",
       "positive    0.281612\n",
       "negative    0.124793\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's create some summary statistics\n",
    "average_characters = round(df['text'].str.len().mean())\n",
    "average_words = round(np.array([len(sentence.split()) for sentence in df['text'].values]).mean())\n",
    "print('Average character count:', average_characters)\n",
    "print('Average word count:', average_words)\n",
    "\n",
    "df['label'].value_counts() / df['label'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  According to Gran , the company has no plans t...\n",
       "1      0  Technopolis plans to develop in stages an area...\n",
       "2      2  The international electronic industry company ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].replace('neutral', 0, inplace=True)\n",
    "df['label'].replace('positive', 1, inplace=True)\n",
    "df['label'].replace('negative', 2, inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and Tokenizing the Data\n",
    "\n",
    "As mentioned preivously, we have to add special tokens to the start and end of each sentence - `[SEP]` and `[CLS`] respectively. Subsequently, we pad and truncate all sentences to a pre-defined length, then explicitly differentiate real tokens from padding tokens with the attention mask.\n",
    "\n",
    "BERT has two constraints:\n",
    "\n",
    "1. All sentences must be padded or truncated to a single, fixed length\n",
    "2. The maximum sequence length is 512 tokens\n",
    "\n",
    "Padding is done with the `[PAD]` token, which is of index 0 in the BERT vocabulary. The attention mask is a simple binary array indicating which tokens are padding and which ones are not. This mask tells the previously described 'self-attention' mechanism in BERT *not* to incorporate the `[PAD]` tokens into its interpretation of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialPhrasebank(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        target = self.targets[item]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we should create split the data using `train_test_split`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4356, 2), (242, 2), (242, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=42)\n",
    "\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = FinancialPhrasebank(\n",
    "        texts=df['text'].to_numpy(),\n",
    "        targets=df['label'].to_numpy().astype(int),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(ds, batch_size=batch_size)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 120\n",
    "\n",
    "train_dataloader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_dataloader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_dataloader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'input_ids', 'attention_mask', 'targets'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_dataloader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 120])\n",
      "torch.Size([16, 120])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### BertForSequenceClassification\n",
    "\n",
    "The documentation for the BERT models under transformers can be found [here](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html). I will be using `BertForSequenceClassification`, which is a normal BERT model with an added linear layer on top for classification. The documentation for `from_pretrained` can be found [here](https://huggingface.co/transformers/v2.2.0/main_classes/model.html#transformers.PreTrainedModel.from_pretrained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=3,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "model.cuda() # use GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Learning Rate Scheduler\n",
    "\n",
    "Suggested values from the paper:\n",
    "- **Batch size:** 16, 32\n",
    "- **Learning rate (Adam):** 5e-5, 3e-5, 2e-5\n",
    "- **Number of epochs:** 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 2\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions to record training metrics\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Epoch 1 / 2 ==========\n",
      "  Batch    40  of    273.    Elapsed: 0:00:08.\n",
      "  Batch    80  of    273.    Elapsed: 0:00:16.\n",
      "  Batch   120  of    273.    Elapsed: 0:00:24.\n",
      "  Batch   160  of    273.    Elapsed: 0:00:32.\n",
      "  Batch   200  of    273.    Elapsed: 0:00:39.\n",
      "  Batch   240  of    273.    Elapsed: 0:00:47.\n",
      "\n",
      "  Average training loss: 0.56\n",
      "  Epoch duration: 0:00:53\n",
      "\n",
      " Running Validation...\n",
      "  Accuracy: 0.82\n",
      "  Validation Loss: 0.41\n",
      "  Validation Duration: 0:00:01\n",
      "\n",
      "========== Epoch 2 / 2 ==========\n",
      "  Batch    40  of    273.    Elapsed: 0:00:08.\n",
      "  Batch    80  of    273.    Elapsed: 0:00:15.\n",
      "  Batch   120  of    273.    Elapsed: 0:00:23.\n",
      "  Batch   160  of    273.    Elapsed: 0:00:31.\n",
      "  Batch   200  of    273.    Elapsed: 0:00:39.\n",
      "  Batch   240  of    273.    Elapsed: 0:00:46.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Epoch duration: 0:00:53\n",
      "\n",
      " Running Validation...\n",
      "  Accuracy: 0.85\n",
      "  Validation Loss: 0.44\n",
      "  Validation Duration: 0:00:01\n",
      "\n",
      " Training complete!\n",
      "Total training took 0:01:48 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')    \n",
    "    \n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []  # store training stats\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    print(\"\\n========== Epoch {:} / {:} ==========\".format(epoch_i + 1, epochs))\n",
    "    t0 = time.time()\n",
    "    total_training_loss = 0\n",
    "    \n",
    "    model.train()  # turn on training mode; doesn't actually start training\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # update progress every 40 batches\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            \n",
    "        # unpack training batch from dataloader, and copy each tensor to the GPU\n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['targets'].to(device)\n",
    "        \n",
    "        # clear previously calculated gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # perform forward pass\n",
    "        loss, logits = model(b_input_ids,\n",
    "                             token_type_ids=None,\n",
    "                             attention_mask=b_input_mask,\n",
    "                             labels=b_labels)\n",
    "        \n",
    "        # accumulate training loss\n",
    "        total_training_loss += loss.item()\n",
    "        \n",
    "        # perform backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # clip the norm of the gradients to 1.0, which prevents exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # update parameters and take a step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "    avg_training_loss = total_training_loss / len(train_dataloader)\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"\\n  Average training loss: {0:.2f}\".format(avg_training_loss))\n",
    "    print(\"  Epoch duration: {:}\".format(training_time))\n",
    "\n",
    "    # ===============================\n",
    "    #           Validation\n",
    "    # ===============================\n",
    "    print(\"\\n Running Validation...\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    # turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['targets'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            (loss, logits) = model(b_input_ids,\n",
    "                                   token_type_ids=None,\n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    # report final accuracy for this validation run\n",
    "    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation Duration: {:}\".format(validation_time))\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_training_loss,\n",
    "            'Validation Loss': avg_val_loss,\n",
    "            'Validation Accuracy': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\\n Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels...\n",
      "Completed.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels...')\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in test_dataloader:\n",
    "    b_input_ids = batch['input_ids'].to(device)\n",
    "    b_input_mask = batch['attention_mask'].to(device)\n",
    "    b_labels = batch['targets'].to(device)\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('Completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Matthews correlation coefficient (MCC)\n",
    "\n",
    "The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary and multiclass classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes.\n",
    "\n",
    "The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. The statistic is also known as the phi coefficient.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{MCC} = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n",
    "\\end{equation}\n",
    "\n",
    "In this equation, TP is the number of true positives, TN the number of true negatives, FP the number of false positives and FN the number of false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "\n",
    "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "    # in to a list of 0s and 1s.\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "\n",
    "    # Calculate and store the coef for this batch.  \n",
    "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "    matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a barplot showing the MCC score for each batch of test samples.\n",
    "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
    "\n",
    "plt.title('MCC Score per Batch')\n",
    "plt.ylabel('MCC Score (-1 to +1)')\n",
    "plt.xlabel('Batch #')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total MCC: 0.799\n"
     ]
    }
   ],
   "source": [
    "# Combine the results across all batches. \n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('Total MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model\n",
    "\n",
    "Both the configuration and the tokenizer can be saved using `save_pretrained()`. In subsequent work, it can be loaded using `from_pretrained()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model_save/vocab.txt',\n",
       " './model_save/special_tokens_map.json',\n",
       " './model_save/added_tokens.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_dir = './model_save/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "8277123/Q3X6FHBI": {
     "URL": "http://arxiv.org/abs/1307.5336",
     "abstract": "The use of robo-readers to analyze news texts is an emerging technology trend in computational finance. In recent research, a substantial effort has been invested to develop sophisticated financial polarity-lexicons that can be used to investigate how financial sentiments relate to future company performance. However, based on experience from other fields, where sentiment analysis is commonly applied, it is well-known that the overall semantic orientation of a sentence may differ from the prior polarity of individual words. The objective of this article is to investigate how semantic orientations can be better detected in financial and economic news by accommodating the overall phrase-structure information and domain-specific use of language. Our three main contributions are: (1) establishment of a human-annotated finance phrase-bank, which can be used as benchmark for training and evaluating alternative models; (2) presentation of a technique to enhance financial lexicons with attributes that help to identify expected direction of events that affect overall sentiment; (3) development of a linearized phrase-structure model for detecting contextual semantic orientations in financial and economic news texts. The relevance of the newly added lexicon features and the benefit of using the proposed learning-algorithm are demonstrated in a comparative study against previously used general sentiment models as well as the popular word frequency models used in recent financial studies. The proposed framework is parsimonious and avoids the explosion in feature-space caused by the use of conventional n-gram features.",
     "accessed": {
      "day": 16,
      "month": 10,
      "year": 2020
     },
     "author": [
      {
       "family": "Malo",
       "given": "Pekka"
      },
      {
       "family": "Sinha",
       "given": "Ankur"
      },
      {
       "family": "Takala",
       "given": "Pyry"
      },
      {
       "family": "Korhonen",
       "given": "Pekka"
      },
      {
       "family": "Wallenius",
       "given": "Jyrki"
      }
     ],
     "container-title": "arXiv:1307.5336 [cs, q-fin]",
     "id": "8277123/Q3X6FHBI",
     "issued": {
      "day": 23,
      "month": 7,
      "year": 2013
     },
     "note": "arXiv: 1307.5336",
     "shortTitle": "Good Debt or Bad Debt",
     "title": "Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts",
     "title-short": "Good Debt or Bad Debt",
     "type": "article-journal"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": "3",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
